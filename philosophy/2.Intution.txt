So it all started with a simple search for audio formats. I did a bit of googling and found out that the wave format is one of the simplest 
sound formats with pcm data. So I thought of making some android app to record voice and store it form of raw pcm data. Well, in the beginning 
it wasn't as easy as I thought it wouldbe... Still i ended up making the app. Next I thought of using java as  the main language as it would 
be easier to port it to my phone. So out of some madness, I took pcm files from the phones sd card and put it in my pc whereI had a java file 
converting the pcm data to wave files. It took me some hours to debug it(Little endian, you sneaky bastard).

After a bit of wave generation, I made a chorus app. So many people would record their voices at same frequencies for a duration seperately and 
in the end the chorus app would combine them to give a chorus recording. On the ground it wasn't much work as I was just adding up the bits to 
get combined results. 

The chorus app made me wonder about visualizations for sound formats and so I took a basic Digital Signal Processing Course from mathworks site.
Matlab proved to be a wonderful to help me visualize the amplitudes and frequencies. Then i started taking multiple recordings of the same word 
with a few varying words at a time to observe the change in data visualizations. The amplitude data always formed some sort of a ECG on steroids 
visuals, while the frequency domain looked like buildings in the manhattan skyline. 

So what I did in the beginning was to convert the wave files from matlab into arrays and write them to txt files. i decided to statistacally analyze 
these files for fun. So decided to take out their means,medians,standard deviations etc. Since most of the amplitude data was evenly distributed 
about the horizontal axis, thus the mean of amplitude data was always near to zero with a significant standard deviation. This step helped us in 
SEGMENATION.




